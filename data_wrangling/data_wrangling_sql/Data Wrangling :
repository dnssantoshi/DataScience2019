Data Wrangling :

1. Import raw data
2. Set Index
3. Handle null values    
4. Fix data types
	1. Span the use of category type
	2. Deal with coerced types
	3. Handle datetime objects 
5. Handle duplicates
6. Normalize embedded json values 
7. Reshape using melt to create a tidy data 


1. Importing data from a csv file :

The first step involved was to import the raw data from file to a dataframe using pandas read_csv() utility.

Examined the imported dataframe using the following:

.info() - This gives a high level summary about the data that lies in the dataframe. For example - the type of data in each column, the no. of non null entries. 

flipkart_df.info()

.columns - can get details on the column names and the no.of columns in the dataset. This was useful to clean up the column names - for example - If the column names have unncessary spaces or can reassign new column names to be precise on what kind of info is depicted by that column.

flipkart_df.columns

.describe() - this gives a high level summary statistics of all quantitative columns 

flipkart_df.describe()

.value_counts() - to identify frquency counts for categorical data

flipkart_df.product_category_tree.value_counts()


.head() - examined the first few rows, to get an gist of type of values for each column. Sometimes, null values can be identified at much early stages.

flipkart_df.head()


.unique() - To get the unique values of a particular column. For instance, if a column has null values and we are not sure about how the null values are stored, we can identify it using this.

flipkart_df.product_name.unique()

.value_counts() - To get the frequency count for each type of value in the column.

2. Identifying the Index:

After examining the raw dataset using the above methods, if a particular column has unique identifier for each row/case/observation/subject then this can be conviniently used to set as index leveraging pandas set_index() method or can be performed while doing the read_csv() import with index_col attribute.

Reference flipkart_df Index : "uniq_id"

flipkart_df = pd.read_csv("../data/flipkart_com-ecommerce_sample.csv",index_col=0)

flipkart_df.set_index('uniq_id',inplace=True)



3. Handle "null" values :

These can often be represented as - empty strings, NaN's, NA, 'missing' or in any other format. Can be examined by using head and unique methods or values attribute.

Following can be used :
1. dropna() - not a great choice since most of the data is lost
2. fillna() - can broadcast null's with a particular value
3. python functions - can fill in custom generated values
4. broadcast using dict comprehensions
5. .map() - using other column data as a basis to fill in the null values.

Reference flipkart_df Index : "brand"

flipkart_df['brand']=[str(k).split()[0] if v is np.NaN else k for k,v in zip(flipkart_df['product_name'],flipkart_df['brand'])] 

// Example for using .map()
dataframe['name'] = dataframe['code'].map({k: v for k, v in zip(dataframe['code'], dataframe['name']) if v is not ''})

5. Fix Datatypes : examine using the info() method 

1. Span the use of category type

If a column has categorical data and its datatype is 'object', this can be changed to 'category' type since its appropriate to use for memory efficiency. This can be done using .astype('category')

Reference flipkart_df Column : "product_category_tree"

flipkart_df.product_category_tree=flipkart_df.product_category_tree.str.replace('[','').str.replace(']','').str.split('>>').str[0].str.strip('\"').astype('category')               


2. Deal with coerced types

If a numeric column (for eg: price) is of dtype: object then it means - the data of that column needs to be examined further. This is often the case that some of the missing values may be set to some string(eg: 'missing') instead of NaN, as a result the data type is coerced to 'object' in other words, string. 

The above can be handled by using the .to_numeric(). But, before using this conversion "null" values need to be handled using any of the techniques above

 
3. Handling datetime objects :

To make data analysis easier, the date columns  need to imported as pandas datetime objects. We can do this using pandas to_datatime() method or can be performed while doing the read_csv() import with parse_dates attribute.

Reference flipkart_df Column : "crawl_timestamp"

flipkart_df['crawl_timestamp']=pd.to_datetime(flipkart_df['crawl_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S'))


5. Handling duplicates :

Duplicates are removed using the drop_duplicates() method. If dropping is based only on a particular column, that can be provided as an value to the subset attribute.
 
For example : in the flipkart_df - the "pid" column has duplicate entries which has the same product details. In this case we can safely discard the duplicate entries retaining one entry and removing the duplicate entry. 

Reference flipkart_df Column : "product_category_tree"

flipkart_df = flipkart_df.drop_duplicates(subset='pid',keep='first') 

6. Normalize embedded json values :

Some of the column values may contain embedded json as their which is required to normalize to make the dataset tidy and ready for analysis.

For example:  flipkart_df.product_specifications[0]

{'product_specification': [{'key': 'Number of Contents in Sales Package',
   'value': 'Pack of 3'},
  {'key': 'Fabric', 'value': 'Cotton Lycra'},
  {'key': 'Type', 'value': 'Cycling Shorts'},
  {'key': 'Pattern', 'value': 'Solid'},
  {'key': 'Ideal For', 'value': "Women's"},
  {'value': 'Gentle Machine Wash in Lukewarm Water, Do Not Bleach'},
  {'key': 'Style Code', 'value': 'ALTHT_3P_21'},
  {'value': '3 shorts'}]}


7. Reshape using melt to create a tidy data 

  Above dataframe needs to be reshaped using .melt() so the rows contain observations and columns contain variables.

























